{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a6a106c2-f185-492f-87bb-79286c2eacc4",
      "metadata": {
        "id": "a6a106c2-f185-492f-87bb-79286c2eacc4"
      },
      "source": [
        "### __Group Project - Predicting Airbnb Listing Prices in Melbourne, Australia__\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad30df24-b18b-4f61-975f-ceebe2e2c3ab",
      "metadata": {
        "id": "ad30df24-b18b-4f61-975f-ceebe2e2c3ab"
      },
      "source": [
        "## Data Cleaning, Missing Observations and Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f2b9f8c-06e2-4d1b-b4bd-d49d82e83de8",
      "metadata": {
        "id": "3f2b9f8c-06e2-4d1b-b4bd-d49d82e83de8"
      },
      "source": [
        "#### Structured Function-Based Data Preprocessing\n",
        "\n",
        "Instead of writing multiple code lines to clean data, I decided to use a more structured and systematic approach by creating functions for each step of data preprocessing. This approach is expected to provide more critical advantages for a comprehensive machine learning pipeline development.\n",
        "\n",
        "**1. Prevents Data Leakage**.\n",
        "All preprocessing parameters, especially for data imputation are computed exclusively from training data and applied consistently to test data. This ensures no future information leaks into model training.\n",
        "\n",
        "**2. Modular and Maintainable**.\n",
        "Each preprocessing step is encapsulated in a function with single responsibility. This makes debugging easier, enables targeted modifications, and improves code readability for team collaboration.\n",
        "\n",
        "**3. Reproducible and Scalable**.\n",
        "The structured pipeline can be systematically applied to new datasets, unit tested independently, and easily extended with additional preprocessing steps as needed.\n",
        "\n",
        "**4. Quality Control**.\n",
        "Function-based validation at each step enables early error detection and ensures consistent transformations across all data splits, leading to more reliable model performance.\n",
        "\n",
        "---\n",
        "\n",
        "To begin, we start by import necessary libraries and load the datasets to start data preparation process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2d9ab53f-f51f-46c6-9d8d-532223dd12c0",
      "metadata": {
        "id": "2d9ab53f-f51f-46c6-9d8d-532223dd12c0",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "\n",
        "from math import sqrt, sin, cos, asin, radians\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# --- Data Loading ---\n",
        "def load_dataset(path: str) -> pd.DataFrame:\n",
        "    return pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003c0066-1df0-4f4c-b852-2f7d3c4b9568",
      "metadata": {
        "id": "003c0066-1df0-4f4c-b852-2f7d3c4b9568"
      },
      "source": [
        "**Approach 1: Numerical Feature Cleaning**\n",
        "\n",
        "• **Converting percentage columns**: Removing '%' symbols from `host_response_rate` and `host_acceptance_rate`, converting to decimal format (e.g., \"85%\" → 0.85).\n",
        "\n",
        "• **Cleaning price data**: Removing ' and ',' symbols from `price` column to extract numerical values (e.g., \"$1,250\" → 1250.0).\n",
        "\n",
        "• **Processing bathroom text**: Converting mixed text in `bathrooms` column by replacing \"half\" with \"0.5\" and extracting numerical values using regex.\n",
        "\n",
        "• **Standardizing data types**: Converting all cleaned values to float type for consistent numerical format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "32485e18",
      "metadata": {
        "id": "32485e18"
      },
      "outputs": [],
      "source": [
        "# --- Feature Cleaning ---\n",
        "def clean_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean numerical features by extracting numerical values and removing text.\n",
        "    Specifically handles 'host_response_rate', 'host_acceptance_rate', 'price',\n",
        "    and 'bathrooms' columns to convert them from mixed text/numeric format to pure numeric values.\n",
        "    \"\"\"\n",
        "    percent_cols = ['host_response_rate', 'host_acceptance_rate']\n",
        "    for col in percent_cols:\n",
        "        if col in df.columns and df[col].dtype == 'object':\n",
        "            df[col] = df[col].str.replace('%', '').astype(float) / 100\n",
        "\n",
        "    if 'price' in df.columns and df['price'].dtype == 'object':\n",
        "       df['price'] = df['price'].str.replace('[\\$,]', '', regex=True).astype(float)\n",
        "\n",
        "    if 'bathrooms' in df.columns and df['bathrooms'].dtype == 'object':\n",
        "        df['bathrooms'] = df['bathrooms'].str.replace('half', '0.5', case=False)\n",
        "        df['bathrooms'] = df['bathrooms'].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23fde9b6-bed0-4018-9edb-3553f3d1fa13",
      "metadata": {
        "id": "23fde9b6-bed0-4018-9edb-3553f3d1fa13"
      },
      "source": [
        "**Approach 2: Feature Engineering from Multi-Information Columns**\n",
        "\n",
        "Given: Existing features containing multiple items of information are host_verifications, and amentities so new features will be created from these two.\n",
        "\n",
        "- **Feature 1 - `verification_count`**: Counting the number of host verification methods from `host_verifications` column by splitting comma-separated values.\n",
        "\n",
        "- **Feature 2 - `amenity_count`**: Converting `amenities` column into total count of available amenities by splitting the comma-separated list.\n",
        "\n",
        "- **Features 3-9 - Luxury amenity indicators**: Creating seven binary features (0/1) for specific amenities:\n",
        "  - `has_pool`: Pool or hot tub availability\n",
        "  - `has_heating_cooling`: Air conditioning or heating systems\n",
        "  - `has_coffee_maker`: Coffee-making equipment\n",
        "  - `has_parking`: Any type of parking availability\n",
        "  - `has_private_balcony`: Private outdoor space\n",
        "  - `has_high_quality_bedding`: Premium bedding amenities\n",
        "  - `has_scenic_view`: Properties with views  \n",
        "\n",
        "- **Feature 10 - `luxury_amenity_count`**: Summing all luxury amenity indicators to create a composite luxury score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c0d210de",
      "metadata": {
        "id": "c0d210de"
      },
      "outputs": [],
      "source": [
        "# --- New Feature Creating ---\n",
        "def create_new_features(df):\n",
        "    \"\"\"\n",
        "    Create new features from existing multi-information columns (host_verifications and amenities).\n",
        "    Extracts counts and key indicators to convert complex text data into usable numerical features.\n",
        "    \"\"\"\n",
        "    # Feature 1: Count of host verification methods\n",
        "    if 'host_verifications' in df.columns:\n",
        "        df['verification_count'] = df['host_verifications'].apply(lambda x: len(x.split(',')) if x else 0)\n",
        "\n",
        "    if 'amenities' in df.columns:\n",
        "        # Feature 2: Total amenity count\n",
        "        df['amenity_count'] = df['amenities'].str.split(', ').apply(len)\n",
        "\n",
        "        # Feature 3-9: Has luxury amenities and scenic view\n",
        "        luxury_keywords = {\n",
        "            'has_pool': ['pool', 'hot tub'],\n",
        "            'has_heating_cooling': ['air conditioning', 'ac', 'heating', 'central heating'],\n",
        "            'has_coffee_maker': ['coffee maker', 'nespresso', 'espresso machine'],\n",
        "            'has_parking': ['free parking', 'paid parking', 'garage', 'carport', 'street parking'],\n",
        "            'has_private_balcony': ['private patio or balcony', 'balcony', 'terrace'],\n",
        "            'has_high_quality_bedding': ['high quality linens', 'extra pillows and blankets',\n",
        "                                         'room-darkening shades', 'blackout curtains',\n",
        "                                         'comfortable bedding', 'premium linens'],\n",
        "            'has_scenic_view': ['view']  # generic catch-all for anything containing 'view'\n",
        "        }\n",
        "\n",
        "        for col, keywords in luxury_keywords.items():\n",
        "            pattern = r'|'.join(rf'\\b{re.escape(kw)}\\b' for kw in keywords)\n",
        "            df[col] = df['amenities'].str.contains(pattern, case=False, regex=True).astype(int)\n",
        "\n",
        "        # Feature 4: Count of luxury amenities\n",
        "        df['luxury_amenity_count'] = df[list(luxury_keywords.keys())].sum(axis=1)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbb16b1d-f4f1-438e-a5af-ae9cc3c04467",
      "metadata": {
        "id": "dbb16b1d-f4f1-438e-a5af-ae9cc3c04467"
      },
      "source": [
        "**Approach 3: Missing Value Imputation**\n",
        "\n",
        "- **Text columns imputation**: Filling missing values in descriptive text columns (`name`, `description`, `neighborhood_overview`, `host_about`) with default text \"no information\" to maintain data completeness.\n",
        "\n",
        "- **Categorical columns imputation**: Using mode (most frequent value) from training data to fill missing values in categorical columns like `host_response_time`, `host_is_superhost`, `property_type`, and `room_type`.\n",
        "\n",
        "- **Numerical columns imputation**: Filling missing values in numerical columns (review scores, rates, counts) with mean values calculated from the training dataset to preserve statistical properties.\n",
        "\n",
        "- **Date-related columns imputation**: For `days_since_first_review` and `days_since_last_review`, using maximum values to handle properties without review history by assuming they are new listings.\n",
        "\n",
        "- **Training-based imputation**: Using statistics from training data (mode/mean) for both training and test datasets to prevent data leakage and maintain consistency across datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "36412d68-f68f-4764-aa6f-88ba33116ef3",
      "metadata": {
        "id": "36412d68-f68f-4764-aa6f-88ba33116ef3"
      },
      "outputs": [],
      "source": [
        "# --- Missing Value Imputation ---\n",
        "def impute_missing(df: pd.DataFrame, train_df: pd.DataFrame, fill_text='no information') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Generalized missing value imputation based on inferred data types and characteristics.\n",
        "    - Text columns: filled with constant text\n",
        "    - Categorical columns: filled with mode from training\n",
        "    - Numeric columns (generalized): filled with mean from training\n",
        "    - Date-like columns: filled with max from test\n",
        "    \"\"\"\n",
        "    # Explicit imputation for known text columns\n",
        "    text_cols = ['name', 'description', 'neighborhood_overview', 'host_about']\n",
        "    for col in [c for c in text_cols if c in df.columns]:\n",
        "        df[col].fillna(fill_text, inplace=True)\n",
        "\n",
        "    # Explicit imputation for known categorical columns\n",
        "    cat_cols = ['host_response_time', 'host_is_superhost', 'host_location',\n",
        "                'host_neighbourhood', 'property_type', 'room_type', 'neighbourhood_cleansed']\n",
        "    for col in [c for c in cat_cols if c in df.columns]:\n",
        "        df[col].fillna(train_df[col].mode()[0], inplace=True)\n",
        "\n",
        "    # Generalized numeric + date-like handling\n",
        "    for col in df.columns:\n",
        "        if df[col].isnull().sum() == 0:\n",
        "            continue # Skip columns with no missing values\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            # If column name suggests it's date-related but stored as numeric, use max value (e.g. \"days_since_*\")\n",
        "            if 'days_since' in col or 'date' in col:\n",
        "                df[col].fillna(df[col].max(), inplace=True)\n",
        "\n",
        "            # Otherwise, use mean from training set; fallback to current df mean if missing\n",
        "            else:\n",
        "                df[col].fillna(train_df[col].mean(), inplace=True)\n",
        "\n",
        "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
        "            df[col].fillna(df[col].max(), inplace=True)\n",
        "\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3ad3f20-7549-4b9e-8894-818c976d1457",
      "metadata": {
        "id": "d3ad3f20-7549-4b9e-8894-818c976d1457"
      },
      "source": [
        "**Approach 4: Categorical Variable Encoding**\n",
        "\n",
        "##### 4.1 Amenities Encoding\n",
        "- **JSON parsing**: Converting amenities from JSON-like strings into lists for processing.\n",
        "- **Top-5 selection**: Identifying the 5 most frequent amenities from training data to create binary features.\n",
        "- **Binary encoding**: Creating `has_[amenity_name]` features (0/1) for each top-5 amenity.\n",
        "- **Other category**: Adding `has_other_amenities` binary feature for properties with amenities outside the top-5.\n",
        "\n",
        "##### 4.2 Neighbourhood Encoding\n",
        "- **Multi-criteria selection**: Choosing neighbourhoods based on three criteria:\n",
        "  - Top 10 most frequent neighbourhoods\n",
        "  - Top 10 highest-priced neighbourhoods  \n",
        "  - Top 10 lowest-priced neighbourhoods\n",
        "- **Binary encoding**: Creating `neigh_[neighbourhood_name]` binary features for selected neighbourhoods.\n",
        "- **Implicit other handling**: Neighbourhoods not selected are implicitly grouped as \"other\" (all binary features = 0).\n",
        "\n",
        "##### 4.3 Other Features Encoding\n",
        "- **Ordinal encoding**: Converting `host_response_time` to numerical scale (within an hour=4, few hours=3, within a day=2, few days or more=1).\n",
        "- **Boolean conversion**: Mapping 't'/'f' values to 1/0 for columns like `host_is_superhost`, `instant_bookable`.\n",
        "- **One-hot encoding**: Creating dummy variables for `bath_type` and `room_type`.\n",
        "- **Top-5 + Other**: For `property_type`, limiting to 5 most frequent categories from training data, grouping remaining as \"Other\", then one-hot encoding.\n",
        "\n",
        "##### Key Principles Applied\n",
        "- **Training-based statistics**: Using training data frequencies to determine top categories for consistent encoding.\n",
        "- **Handling rare categories**: Grouping infrequent values into \"Other\" category as specified.\n",
        "- **Consistent encoding**: Applying same encoding rules to both training and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f91f4448-8574-4397-a636-d83bbde885e2",
      "metadata": {
        "id": "f91f4448-8574-4397-a636-d83bbde885e2"
      },
      "outputs": [],
      "source": [
        "# --- 4.1 Amenities Encoding ---\n",
        "def encode_amenities(df: pd.DataFrame, train_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create binary features for the top 5 most common amenities from training data.\n",
        "    Parses amenities from JSON-like strings, identifies the most frequent amenities,\n",
        "    and creates indicator variables for popular amenities plus a catch-all for other amenities.\n",
        "    \"\"\"\n",
        "    def parse(amenities):\n",
        "        try:\n",
        "            if isinstance(amenities, str):\n",
        "                return json.loads(amenities.replace(\"'\", '\"'))\n",
        "            return []\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "    if 'amenities' in df.columns:\n",
        "        # Parse amenities for both dataframes\n",
        "        df['amenities_list'] = df['amenities'].apply(parse)\n",
        "        train_df['amenities_list'] = train_df['amenities'].apply(parse)\n",
        "\n",
        "        # Get top 5 amenities from training data\n",
        "        all_items = [item for sublist in train_df['amenities_list'] for item in sublist]\n",
        "        top_5 = pd.Series(all_items).value_counts().nlargest(5).index.tolist()\n",
        "\n",
        "        # Create binary features for top 5 amenities\n",
        "        for amenity in top_5:\n",
        "            key = f'has_{amenity.lower().replace(\" \", \"_\")}'\n",
        "            df[key] = df['amenities_list'].apply(lambda x: int(amenity in x))\n",
        "\n",
        "        # Create feature for other amenities\n",
        "        df['has_other_amenities'] = df['amenities_list'].apply(lambda x: int(any(i not in top_5 for i in x)))\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- 4.2 Location Encoding ---\n",
        "def encode_neighbourhoods(df: pd.DataFrame, train_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if 'neighbourhood_cleansed' not in df.columns:\n",
        "        return df\n",
        "    top_freq = train_df['neighbourhood_cleansed'].value_counts().nlargest(10).index.tolist()\n",
        "    top_price = train_df.groupby('neighbourhood_cleansed')['price'].mean().nlargest(10).index.tolist()\n",
        "    low_price = train_df.groupby('neighbourhood_cleansed')['price'].mean().nsmallest(10).index.tolist()\n",
        "    selected = list(set(top_freq + top_price + low_price))\n",
        "\n",
        "    for neigh in selected:\n",
        "        df[f\"neigh_{neigh.replace(' ', '_').lower()}\"] = (df['neighbourhood_cleansed'] == neigh).astype(int)\n",
        "    return df\n",
        "\n",
        "# --- 4.3 Other Features Encoding ---\n",
        "def encode_other_features(df: pd.DataFrame, train_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Encode features respectively:\n",
        "    - Ordinal encode host_response_time\n",
        "    - Convert booleans ('t'/'f') to 1/0\n",
        "    - One-hot encode room_type and bath_type\n",
        "    - One-hot encode top-5 property_type (others grouped)\n",
        "    \"\"\"\n",
        "    # Ordinal encode response time\n",
        "    if 'host_response_time' in df.columns:\n",
        "        response_order = {'within an hour': 4, 'within a few hours': 3, 'within a day': 2, 'a few days or more': 1}\n",
        "        df['host_response_time'] = df['host_response_time'].map(response_order).fillna(0).astype(int)\n",
        "\n",
        "    # One-hot encode bath_type and room_type\n",
        "    for col in ['bath_type', 'room_type']:\n",
        "        if col in df.columns:\n",
        "            df = pd.concat([df, pd.get_dummies(df[col], prefix=col)], axis=1)\n",
        "\n",
        "    # Property type: Top-5 + Other one-hot\n",
        "    if 'property_type' in df.columns:\n",
        "        top5 = train_df['property_type'].value_counts().nlargest(5).index\n",
        "        df['property_type_limited'] = df['property_type'].where(df['property_type'].isin(top5), 'Other')\n",
        "        df = pd.concat([df, pd.get_dummies(df['property_type_limited'], prefix='property_type')], axis=1)\n",
        "\n",
        "    # Convert boolean columns\n",
        "    bool_cols = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'instant_bookable']\n",
        "                # this doesn't include 'has_availabity' b/c this column contains only 1 value 't', which will not meaningful to the machine learning process\n",
        "    for col in bool_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].map({'t': 1, 'f': 0}).fillna(-1)\n",
        "\n",
        "    # Final Safe-Net: turn all new introduced bool dtype columns to int\n",
        "        bool_dtype_cols = df.select_dtypes(include='bool').columns\n",
        "        df[bool_dtype_cols] = df[bool_dtype_cols].astype(int)\n",
        "\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f77a56b3-3e55-41ec-aeeb-eeec7bd762c1",
      "metadata": {
        "id": "f77a56b3-3e55-41ec-aeeb-eeec7bd762c1"
      },
      "source": [
        "**Approach 5: Additional Data Preparation Steps**\n",
        "\n",
        "To enhance model performance and capture complex data patterns, the following additional preprocessing steps were applied:\n",
        "1. *Additional Feature Engineering (Sections 5.1-5.3):*\n",
        "    - Applied before imputation (Q3) to ensure any new missing values introduced can be immediately handled.\n",
        "    \n",
        "        ---\n",
        "2. *Sentiment Analysis (Section 5.4)*\n",
        "    - Performed after handling missing values (Q3), focusing on text features.\n",
        "        \n",
        "        ---\n",
        "3. *Feature Transformation (Section 5.5)*\n",
        "    - Applied to reduce feature bias and improve numerical stability\n",
        "    \n",
        "        ---\n",
        "4. *Pipeline Integration and Execution (Sections 5.6–5.7)*\n",
        "   - Ensures a logically structured and reproducible processing flow.  \n",
        "   - Facilitates debugging, testing, and future enhancements to the preprocessing pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98dab963-a25f-40c6-b4a7-84d60efa3d00",
      "metadata": {
        "id": "98dab963-a25f-40c6-b4a7-84d60efa3d00"
      },
      "source": [
        "**5.1 Extra Feature Engineering**\n",
        "\n",
        "**Goal:** Transform raw textual and date fields into structured, informative features.\n",
        "\n",
        "- Bathroom and Room Type Extraction\n",
        "  - `bath_type`: Categorizes bathrooms as 'Shared', 'Private', or 'Normal' based on keywords.\n",
        "  - `room_type`: Standardizes room type entries using keyword matching (e.g., 'entire', 'shared').\n",
        "      \n",
        "      ---\n",
        "        \n",
        "- Date-Based Features\n",
        "  - Converts `host_since`, `first_review`, and `last_review` to datetime.\n",
        "  - Derived features:\n",
        "    - `host_experience_days`: Days since host joined.\n",
        "    - `days_since_first_review`: Days since the first review.\n",
        "    - `days_since_last_review`: Days since the last review.\n",
        "    \n",
        "    ---\n",
        "        \n",
        "- Popularity Metrics\n",
        "  - `reviews_per_day`: Measures listing popularity as number_of_reviews ÷ host_experience_days.\n",
        "        \n",
        "      ---\n",
        "        \n",
        "- Temporal Scaling\n",
        "  - `review_recency_score`: Recency scaled with exponential decay to favor recent reviews.\n",
        "  - `host_experience_maturity`: Log-transformed host experience for diminishing returns effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "21a238a3-5432-4058-8b6e-1172c4e396ad",
      "metadata": {
        "id": "21a238a3-5432-4058-8b6e-1172c4e396ad"
      },
      "outputs": [],
      "source": [
        "# --- 5.1 Creating Extra Features ---\n",
        "def add_extra_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create additional engineered features from existing data including bathroom/room type extraction,\n",
        "    date-based features for host experience and review timing, popularity metrics, and temporal scaling.\n",
        "    Transforms raw data into more meaningful features for machine learning models.\n",
        "    \"\"\"\n",
        "    # --- Bath & Room Type Extraction ---\n",
        "    bath_mapping = {'hared': 'Shared', 'rivate': 'Private'}\n",
        "    room_mapping = {'entire': 'Entire', 'shared': 'Shared', 'private': 'Private', 'hotel': 'Hotel'}\n",
        "\n",
        "    if 'bathrooms' in df.columns:\n",
        "        df['bath_type'] = df['bathrooms'].astype(str).apply(\n",
        "            lambda x: next((v for k, v in bath_mapping.items() if k in x), 'Normal')\n",
        "        )\n",
        "        df['bathroom_count'] = df['bathrooms']\n",
        "\n",
        "    if 'room_type' in df.columns:\n",
        "        df['room_type'] = df['room_type'].astype(str).str.lower().apply(\n",
        "            lambda x: next((v for k, v in room_mapping.items() if k in x), 'Unknown')\n",
        "        )\n",
        "\n",
        "    # --- Date Parsing ---\n",
        "    date_cols = ['host_since', 'first_review', 'last_review']\n",
        "    for col in date_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "    # --- Experience & Review Age Features ---\n",
        "    ref_date = max([df[col].max() for col in date_cols if col in df.columns])\n",
        "    feature_ages = {\n",
        "        'host_since': 'host_experience_days',\n",
        "        'first_review': 'days_since_first_review',\n",
        "        'last_review': 'days_since_last_review'\n",
        "    }\n",
        "    for col, new_col in feature_ages.items():\n",
        "        if col in df.columns:\n",
        "            df[new_col] = (ref_date - df[col]).dt.days\n",
        "\n",
        "    df.drop([col for col in date_cols if col in df.columns], axis=1, inplace=True)\n",
        "\n",
        "    # --- Popularity Signal ---\n",
        "    if {'number_of_reviews', 'host_experience_days'}.issubset(df.columns):\n",
        "        df['reviews_per_day'] = df['number_of_reviews'] / df['host_experience_days'].replace(0, np.nan)\n",
        "\n",
        "    # --- Temporal Scaling ---\n",
        "    if 'days_since_last_review' in df.columns:\n",
        "        df['review_recency_score'] = np.exp(-df['days_since_last_review'] / 365)\n",
        "\n",
        "    if 'host_experience_days' in df.columns:\n",
        "        df['host_experience_maturity'] = np.log1p(df['host_experience_days'])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447ec91d-cbd6-4da3-9b0d-1497cd1cad85",
      "metadata": {
        "id": "447ec91d-cbd6-4da3-9b0d-1497cd1cad85"
      },
      "source": [
        "**5.2 Distance-Based Features**\n",
        "\n",
        "**Goal:** Quantify geographic desirability based on proximity to Melbourne landmarks.\n",
        "\n",
        "- Distance Calculations (Haversine Formula)\n",
        "  - `dist_to_cbd`: Distance to Melbourne CBD.\n",
        "  - `dist_to_beach`: Distance to the nearest beach.\n",
        "  - `dist_to_mall`: Distance to the nearest shopping mall.\n",
        "  \n",
        "  ---\n",
        "\n",
        "- Proximity Count\n",
        "  - `count_nearby_sights`: Number of attractions (CBD, malls, beaches) within a 0.8 km radius (equivalent to 10-minute walking distance, a favourable distance for AirBnb tenants)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "5d5d8616-a6c9-4245-ac1d-a1f08b337ff1",
      "metadata": {
        "id": "5d5d8616-a6c9-4245-ac1d-a1f08b337ff1"
      },
      "outputs": [],
      "source": [
        "# --- 5.2 Creating Distance Features ---\n",
        "def add_distance_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate location-based features using haversine distance formula.\n",
        "    Computes distances to Melbourne CBD, nearest beach, nearest shopping mall,\n",
        "    and counts nearby attractions within 0.8km radius to capture location desirability.\n",
        "    \"\"\"\n",
        "    def haversine(lon1, lat1, lon2, lat2):\n",
        "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
        "        dlon, dlat = lon2 - lon1, lat2 - lat1\n",
        "        a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
        "        return 6371 * 2 * asin(sqrt(a))\n",
        "\n",
        "    # Central Business District (CBD) coordinates\n",
        "    cbd = (-37.8136, 144.9631)\n",
        "\n",
        "    # List of major Melbourne beaches\n",
        "    beaches = [\n",
        "        (-37.847039, 144.945525),  # Port Melbourne\n",
        "        (-37.854066, 144.960893),  # South Melbourne / Middle Park\n",
        "        (-37.864000, 144.982000),  # St Kilda\n",
        "        (-37.892639, 144.988190),  # Elwood\n",
        "        (-37.905000, 144.996000),  # Brighton (Dendy St)\n",
        "        (-37.937000, 145.009000),  # Hampton\n",
        "        (-37.952500, 145.012311),  # Sandringham\n",
        "        (-37.989640, 145.058720),  # Mentone\n",
        "        (-38.000000, 145.083330),  # Mordialloc\n",
        "        (-38.042559, 145.109469),  # Edithvale\n",
        "        (-38.053029, 145.113617),  # Chelsea\n",
        "        (-38.083330, 145.133330),  # Carrum\n",
        "        (-38.100000, 145.133330),  # Seaford\n",
        "        (-38.140343, 145.119280),  # Frankston\n",
        "        (-37.870911, 144.830323),  # Altona\n",
        "        (-37.863930, 144.894320)   # Williamstown\n",
        "    ]\n",
        "\n",
        "    # List of popular Melbourne shopping malls\n",
        "    malls = [\n",
        "        (-37.887085, 145.081528),  # Chadstone Shopping Centre\n",
        "        (-37.770700, 144.884800),  # Highpoint Shopping Centre\n",
        "        (-37.958000, 145.050000),  # Westfield Southland\n",
        "        (-37.783333, 145.121667),  # Westfield Doncaster\n",
        "        (-37.868970, 145.241480),  # Westfield Knox\n",
        "        (-38.017833, 145.302500),  # Westfield Fountain Gate\n",
        "        (-37.813060, 145.229170),  # Eastland Shopping Centre\n",
        "        (-37.738330, 145.029720),  # Northland Shopping Centre\n",
        "        (-37.876390, 145.165638),  # The Glen (Glen Waverley)\n",
        "        (-37.812400, 144.963800),  # Emporium Melbourne\n",
        "        (-37.810272, 144.962646),  # Melbourne Central\n",
        "        (-37.810696, 144.965716),  # QV Melbourne\n",
        "        (-37.813002, 144.938031),  # The District Docklands\n",
        "        (-37.875213, 144.679659),  # Pacific Werribee\n",
        "        (-37.652004, 145.023085),  # Pacific Epping\n",
        "        (-37.819560, 145.121410),  # Box Hill Central (South)\n",
        "        (-37.812724, 145.010977),  # Victoria Gardens (Richmond)\n",
        "        (-37.825196, 144.949879),  # DFO South Wharf\n",
        "        (-37.733125, 144.906547),  # DFO Essendon\n",
        "        (-37.971261, 145.088120),  # DFO Moorabbin\n",
        "        (-37.651367, 145.071752),  # Westfield Plenty Valley\n",
        "        (-37.699266, 144.775813),  # Watergardens Town Centre\n",
        "        (-37.992893, 145.173138),  # Parkmore Shopping Centre\n",
        "        (-38.141951, 145.124068)   # Bayside Shopping Centre\n",
        "    ]\n",
        "\n",
        "    df['dist_to_cbd'] = df.apply(lambda r: haversine(r['longitude'], r['latitude'], cbd[1], cbd[0]), axis=1)\n",
        "    df['dist_to_beach'] = df.apply(lambda r: min(haversine(r['longitude'], r['latitude'], lon, lat) for lat, lon in beaches), axis=1)\n",
        "    df['dist_to_mall'] = df.apply(lambda r: min(haversine(r['longitude'], r['latitude'], lon, lat) for lat, lon in malls), axis=1)\n",
        "\n",
        "    top_sights = [cbd] + beaches + malls\n",
        "    def count_nearby_sights(row):\n",
        "        return sum(\n",
        "            1 for lat, lon in top_sights\n",
        "            if haversine(row['longitude'], row['latitude'], lon, lat) <= 0.8\n",
        "        )\n",
        "\n",
        "    df['count_nearby_sights'] = df.apply(count_nearby_sights, axis=1)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3700b77c-ec8e-419d-adf1-35ab53fe7b63",
      "metadata": {
        "id": "3700b77c-ec8e-419d-adf1-35ab53fe7b63"
      },
      "source": [
        "**5.3 Interaction Features**\n",
        "\n",
        "**Goal:** Capture important combinations of features that express non-linear relationships.\n",
        "\n",
        "- Superhost Experience\n",
        "  - `superhost_experience`: Interaction of superhost flag and host experience.\n",
        "  \n",
        "    ---\n",
        "- Space Efficiency\n",
        "  - `crowding_index`: Accommodates ÷ (bedrooms + bathrooms) for space utilization.\n",
        "  \n",
        "    ---\n",
        "- Luxury and More Capacity Impact\n",
        "  - `luxury_entire_home`: Product of luxury amenity count and entire-home flag.\n",
        "  \n",
        "    ---\n",
        "- Location-View with Good Rating\n",
        "  - `cbd_view_interaction`: Combines `dist_to_cbd` with view indicator or value rating.\n",
        "  \n",
        "    ---\n",
        "- Rating-Volume Interaction\n",
        "  - `rating_review_volume`: Review score multiplied by reviews per day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "0f52319c-8ffb-4c8d-afc5-723440aa261e",
      "metadata": {
        "id": "0f52319c-8ffb-4c8d-afc5-723440aa261e"
      },
      "outputs": [],
      "source": [
        "# --- 5.3 Creating Interaction Features ---\n",
        "def add_interaction_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create interaction features by combining existing features to capture non-linear relationships.\n",
        "    These features represent meaningful combinations like experienced superhosts, space efficiency,\n",
        "    luxury entire homes, location-view interactions, and rating-popularity synergies.\n",
        "    \"\"\"\n",
        "    if 'is_superhost' in df.columns and 'host_experience_days' in df.columns:\n",
        "        df['superhost_experience'] = df['is_superhost'] * df['host_experience_days']\n",
        "\n",
        "    if 'accommodates' in df.columns and 'bedrooms' in df.columns and 'bathroom_count' in df.columns:\n",
        "        df['crowding_index'] = df['accommodates'] / (df['bedrooms'] + df['bathroom_count']).replace(0, np.nan)\n",
        "\n",
        "    if 'amenity_count' in df.columns and 'room_type_Entire' in df.columns:\n",
        "        df['luxury_entire_home'] = df['luxury_amenity_count'] * df['room_type_Entire']\n",
        "\n",
        "    if 'dist_to_cbd' in df.columns and 'has_scenic_view' in df.columns:\n",
        "        df['cbd_view_interaction'] = df['dist_to_cbd'] * df['has_scenic_view']\n",
        "\n",
        "    if 'dist_to_cbd' in df.columns and 'review_scores_value' in df.columns:\n",
        "        df['cbd_view_interaction'] = df['dist_to_cbd'] * df['review_scores_value']\n",
        "\n",
        "    if 'review_scores_rating' in df.columns and 'reviews_per_day' in df.columns:\n",
        "        df['rating_review_volume'] = df['review_scores_rating'] * df['reviews_per_day']\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b04ac3-1d31-48a3-af57-8d031ab0ee50",
      "metadata": {
        "id": "05b04ac3-1d31-48a3-af57-8d031ab0ee50"
      },
      "source": [
        "**5.4 Sentiment Analysis**\n",
        "\n",
        "**Goal:** Extract emotional tone from descriptive text fields.\n",
        "\n",
        "- **VADER Sentiment Scores**\n",
        "  - Applied to `neighborhood_overview` and `host_about`.\n",
        "  - Outputs:\n",
        "    - `neighborhood_overview_sentiment`\n",
        "    - `host_about_sentiment`\n",
        "  - Compound scores (range -1 to 1) indicate overall sentiment polarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "972b18af-6add-4b18-b5c1-7ff5f286229f",
      "metadata": {
        "id": "972b18af-6add-4b18-b5c1-7ff5f286229f"
      },
      "outputs": [],
      "source": [
        "# --- 5.4 Creating Sentiment Features ---\n",
        "def add_sentiment_scores(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Clean text and apply VADER sentiment analysis to specific columns:\n",
        "    'description', 'neighborhood_overview', and 'host_about'.\n",
        "\n",
        "    Cleaning includes:\n",
        "    - Removing HTML tags\n",
        "    - Removing non-alphanumeric characters (except basic punctuation)\n",
        "    - Trimming extra whitespace\n",
        "\n",
        "    Add sentiment analysis scores to text columns using VADER sentiment analyzer.\n",
        "    Analyzes neighborhood_overview and host_about text to extract sentiment polarity scores,\n",
        "    which can indicate positive or negative sentiment that may influence booking decisions.\n",
        "    \"\"\"\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "\n",
        "    def clean_text(text):\n",
        "        if not isinstance(text, str):\n",
        "            return \"\"\n",
        "        text = re.sub(r'<[^>]+>', ' ', text)  # Remove HTML tags\n",
        "        text = re.sub(r'[^\\w\\s.,!?\\'-]', '', text)  # Remove unwanted non-alphanumerics\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
        "        return text\n",
        "\n",
        "    columns = ['neighborhood_overview', 'host_about']\n",
        "    for col in columns:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].apply(clean_text)\n",
        "            df[f'{col}_sentiment'] = df[col].apply(\n",
        "                lambda x: sid.polarity_scores(x)['compound'] if x else 0.0\n",
        "            )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d94e9459-b66b-4f99-acf6-88659e5b94c3",
      "metadata": {
        "id": "d94e9459-b66b-4f99-acf6-88659e5b94c3"
      },
      "source": [
        "**5.5 Mathematical Feature Transformations**\n",
        "\n",
        "**Goal:** Normalize skewed distributions and better capture non-linear relationships.\n",
        "\n",
        "- **Log1p Transform (Skew Reduction)**\n",
        "  - Applied to variables like `minimum_nights`, `availability_365`, and date differences.\n",
        "  \n",
        "    ---\n",
        "\n",
        "- **Square Root Transform (Diminishing Returns)**\n",
        "  - Used on count features such as `accommodates`, `bedrooms`, `amenity_count`.\n",
        "  \n",
        "    ---\n",
        "\n",
        "- **Square Transform (Quadratic Relationships)**\n",
        "  - Applied to distance-based features (`dist_to_cbd`, etc.) to capture non-linear distance effects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "19a72963-a743-486a-9302-3fd33f68ba5c",
      "metadata": {
        "id": "19a72963-a743-486a-9302-3fd33f68ba5c"
      },
      "outputs": [],
      "source": [
        "# --- 5.5 Feature Transfromations ---\n",
        "def apply_feature_transformations(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Apply mathematical transformations to features to improve model performance.\n",
        "    Uses log1p for skewed variables, sqrt for diminishing returns features,\n",
        "    and squares for distance features to better capture non-linear relationships.\n",
        "    \"\"\"\n",
        "    # Reduce skew: apply log1p\n",
        "    log1p_cols = [\n",
        "        'minimum_nights', 'maximum_nights', 'minimum_minimum_nights', 'maximum_maximum_nights',\n",
        "        'availability_365', 'host_experience_days', 'days_since_first_review', 'days_since_last_review',\n",
        "        'reviews_per_month', 'dist_to_beach', 'dist_to_mall', 'host_experience_days'\n",
        "    ]\n",
        "    for col in log1p_cols:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_log1p'] = np.log1p(df[col].clip(lower=0))\n",
        "\n",
        "    # Diminishing returns: sqrt\n",
        "    diminishing_cols = ['accommodates', 'bedrooms', 'bathroom_count', 'amenity_count', 'count_nearby_sights']\n",
        "    for col in diminishing_cols:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_sqrt'] = np.sqrt(df[col].clip(lower=0))\n",
        "\n",
        "    # Location non-linearity: square\n",
        "    distance_cols = ['dist_to_cbd', 'dist_to_beach', 'dist_to_mall']\n",
        "    for col in distance_cols:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_sq'] = df[col] ** 2\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afeed94-78cb-40e7-953e-08d2416c3985",
      "metadata": {
        "id": "3afeed94-78cb-40e7-953e-08d2416c3985"
      },
      "source": [
        "**5.6-5.7 Pipeline Integration**\n",
        "\n",
        "**Goal:** Maintain consistency, avoid leakage, and ensure reproducibility.\n",
        "\n",
        "- Steps are **sequentially applied** in the correct logical order:\n",
        "  1. Feature extraction (bath/date-based)\n",
        "  2. Distance and interaction features\n",
        "  3. Missing value imputation\n",
        "  4. Categorical encoding\n",
        "  5. Sentiment scoring\n",
        "  6. Final transformations\n",
        "  \n",
        "  ---\n",
        "\n",
        "- Ensures **identical preprocessing** for both training and test sets using training-derived stats.\n",
        "- Verifies column alignment and handles column drops/exports systematically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6d011052-a5fe-43f1-b63d-7ea4464cd6c4",
      "metadata": {
        "id": "6d011052-a5fe-43f1-b63d-7ea4464cd6c4",
        "outputId": "a20d30b1-730b-46fc-9747-309814efb5ae"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Trung\\AppData\\Local\\Temp\\ipykernel_5988\\1040662810.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{col}_sqrt'] = np.sqrt(df[col].clip(lower=0))\n",
            "C:\\Users\\Trung\\AppData\\Local\\Temp\\ipykernel_5988\\1040662810.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{col}_sq'] = df[col] ** 2\n",
            "C:\\Users\\Trung\\AppData\\Local\\Temp\\ipykernel_5988\\1040662810.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{col}_sq'] = df[col] ** 2\n",
            "C:\\Users\\Trung\\AppData\\Local\\Temp\\ipykernel_5988\\1040662810.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{col}_sq'] = df[col] ** 2\n",
            "C:\\Users\\Trung\\AppData\\Local\\Temp\\ipykernel_5988\\1040662810.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{col}_sq'] = df[col] ** 2\n",
            "C:\\Users\\Trung\\AppData\\Local\\Temp\\ipykernel_5988\\1040662810.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{col}_sq'] = df[col] ** 2\n",
            "C:\\Users\\Trung\\AppData\\Local\\Temp\\ipykernel_5988\\1040662810.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[f'{col}_sq'] = df[col] ** 2\n"
          ]
        }
      ],
      "source": [
        "# --- 5.6 Cleanup & Export ---\n",
        "def finalize_and_export(df: pd.DataFrame, output_path: str):\n",
        "    \"\"\"\n",
        "    Removes unnecessary columns from the DataFrame and exports the cleaned dataset to CSV.\n",
        "    \"\"\"\n",
        "    drop_cols = [\n",
        "        'source', 'name', 'host_name', 'description', 'neighborhood_overview', 'property_type_limited',\n",
        "        'host_about', 'host_location',  'host_neighbourhood', 'neighbourhood', 'host_verifications', 'bathrooms',\n",
        "        'amenities', 'amenities_list', 'property_type', 'bath_type', 'room_type', 'has_availability', 'neighbourhood_cleansed'\n",
        "    ]\n",
        "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
        "    df.to_csv(output_path, index=False)\n",
        "    return df\n",
        "\n",
        "# --- 5.7 Run Pipeline ---\n",
        "def preprocess(df: pd.DataFrame, ref_df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Applies all preprocessing steps to transform raw data into model-ready features.\n",
        "    \"\"\"\n",
        "    df = clean_features(df) # Approach 1\n",
        "    df = create_new_features(df) # Approach 2\n",
        "\n",
        "    df = add_extra_features(df) # Approach 5.1 --- add extra features ---\n",
        "    df = add_distance_features(df) # Approach 5.2 --- add distance features ---\n",
        "    df = add_interaction_features(df) # Approach 5.3 --- add interaction features ---\n",
        "\n",
        "    df = impute_missing(df, ref_df) # Approach 3\n",
        "\n",
        "    df = encode_amenities(df, ref_df) # Approach 4\n",
        "    df = encode_neighbourhoods(df, ref_df) # Approach 4\n",
        "    df = encode_other_features(df, ref_df) # Approach 4\n",
        "\n",
        "    #df = add_sentiment_scores(df) # Approach 5.4 --- sentiment analysis ---\n",
        "    df = apply_feature_transformations(df) # Q5.5 --- feature transformation ---\n",
        "\n",
        "    return df\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"\n",
        "    Loads datasets, applies preprocessing pipeline, and exports cleaned data.\n",
        "    \"\"\"\n",
        "    train_df = load_dataset('train.csv')\n",
        "    test_df = load_dataset('test.csv')\n",
        "\n",
        "    train_df = preprocess(train_df, train_df)\n",
        "    test_df = preprocess(test_df, train_df)\n",
        "\n",
        "    finalize_and_export(train_df, 'train_cleaned.csv')\n",
        "    finalize_and_export(test_df, 'test_cleaned.csv')\n",
        "\n",
        "run_pipeline() # Complete preprocessing data "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
